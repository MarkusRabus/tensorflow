{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion Tensorflow\n",
    "\n",
    "Author: Markus Rabus\n",
    "\n",
    "Este notebook es una introduccion a tensorflow, parte del curso optativo Vision Computacional de doctorado de intelegencia Artificial.\n",
    "\n",
    "En este notebook veamos la estructura basica de programas de tensoflow.\n",
    "\n",
    "Lo primero es importar packetes utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 10:18:04.577424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar quieremos minimizar la función (función de costo):\n",
    "\n",
    "$J(w) = w^2 - 14 x + 49$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función a minimizar\n",
    "def func(w):\n",
    "    return w**2 - 14*w + 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la variable w como un tensor de tipo float32 con valor 0\n",
    "w = tf.Variable(0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente descendiente\n",
    "\n",
    "Despues de definir la variable y la funcion que queremos minimizar (función de costo) tenemos que definir el algoritmo. En este caso usamos el algoritmo gradiente descendiente. Recordamos la funcion de gradiente descente es encontrar nuevos valores para los pesos a traves de encontrar el gradiente.\n",
    "\n",
    "Actualizamos nuestro valor de $w$ a traves de la siguiente funcion:\n",
    "\n",
    "$w_{n+1} = w_n - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "Se actualiza $w$ un cierto numero de iteraciones o hasta converge la funcion hacia el minimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir el optimizador con un learning rate de 0.1\n",
    "optimizer = tf.optimizers.SGD(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el paso para la optimizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimization_step():\n",
    "    # GradientTape es un contexto que registra las operaciones que se realizan sobre los tensores para calcular el gradiente\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = func(w) #Se calcula el loss de la función usando el valor actual de w\n",
    "    \n",
    "    gradients = tape.gradient(loss, [w]) #Se calcula el gradiente de la función con respecto a w\n",
    "    optimizer.apply_gradients(zip(gradients, [w])) #Se actualiza el valor de w con el gradiente calculado    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: w = 1.399999976158142, loss = 49.0\n",
      "Step 1: w = 2.5199999809265137, loss = 31.35999870300293\n",
      "Step 2: w = 3.4159998893737793, loss = 20.07040023803711\n",
      "Step 3: w = 4.132800102233887, loss = 12.84505844116211\n",
      "Step 4: w = 4.706240177154541, loss = 8.220836639404297\n",
      "Step 5: w = 5.164992332458496, loss = 5.2613372802734375\n",
      "Step 6: w = 5.531993865966797, loss = 3.3672561645507812\n",
      "Step 7: w = 5.825594902038574, loss = 2.1550445556640625\n",
      "Step 8: w = 6.060475826263428, loss = 1.3792304992675781\n",
      "Step 9: w = 6.248380661010742, loss = 0.8827018737792969\n"
     ]
    }
   ],
   "source": [
    "#El paso de optimización se repite 10 veces, imprimiendo el valor de w y el loss en cada paso.\n",
    "for step in range(10):\n",
    "    loss = optimization_step()\n",
    "    print(f\"Step {step}: w = {w.numpy()}, loss = {loss.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
